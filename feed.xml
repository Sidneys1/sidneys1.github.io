<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://sidneys1.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://sidneys1.github.io/" rel="alternate" type="text/html" /><updated>2022-10-03T13:02:01-04:00</updated><id>https://sidneys1.github.io/feed.xml</id><title type="html">Sidneys1.com on GitHub Pages</title><subtitle>A home for all my ramblings on subjects such as programming, cybersecurity, photography, videography, video games, and whatever else I see fit.</subtitle><entry><title type="html">PowerShell Profile Instant Prompt</title><link href="https://sidneys1.github.io/programming/2022/09/29/powershell-profile-instant-prompt.html" rel="alternate" type="text/html" title="PowerShell Profile Instant Prompt" /><published>2022-09-29T00:00:00-04:00</published><updated>2022-09-29T00:00:00-04:00</updated><id>https://sidneys1.github.io/programming/2022/09/29/powershell-profile-instant-prompt</id><content type="html" xml:base="https://sidneys1.github.io/programming/2022/09/29/powershell-profile-instant-prompt.html"><![CDATA[Recently I began using [Oh My Posh](https://ohmyposh.dev/) for PowerShell 7+ (pwsh). One thing I noticed however is that
it takes upward of a second to activate in my pwsh `$profile`. Let's dig in and see if we can't improve that.

<!--more-->

First, let's establish a baseline - after [installing Oh My Posh](https://ohmyposh.dev/docs/installation/windows) (say,
with WinGet) we're instructed to add the line `oh-my-posh init pwsh | Invoke-Expression` to our pwsh `$profile`. We can
investigate the cost of this with a handy pwsh package `PSProfiler`:

```powershell
Install-Module PSProfiler;
& pwsh.exe -NoProfile -Command {Import-Module PSProfiler; Measure-Script $profile;}

# Count  Line       Time Taken Statement
# -----  ----       ---------- ---------
#     1     1    00:00.0734463 Import-Module PSReadLine;
#     1     2    00:00.0234282 Set-PSReadLineOption -EditMode Windows
#     1     3    00:00.0011258 Set-PSReadLineOption -PredictionSource HistoryAndPlugin
#     1     4    00:00.0016107 Set-PSReadLineOption -PredictionViewStyle InlineView
#     0     5    00:00.0000000
#     1     6    00:00.3770726 oh-my-posh init pwsh | Invoke-Expression
#     1     7    00:00.0005995 Enable-PoshTransientPrompt
#     1     8    00:00.0005577 Enable-PoshLineError
```

You can see that out of all the commands I have in my profile, oh-my-posh init is taking an order of magnitude longer
than the others. When my system is under load and I _really need that terminal now_, this causes friction. Let's see
what exactly `oh-my-posh init pwsh` is outputting that gets interpreted by `Invoke-Expression` (note that I've inserted
`%LOCALAPPDATA%` and `<some config path>` for brevity):

```powershell
oh-my-posh init pwsh
# (@(& '%LOCALAPPDATA%/Programs/oh-my-posh/bin/oh-my-posh.exe' init pwsh --config='<some config path>' --print) -join "`n") | Invoke-Expression
```

It looks like it just calls itself again! We can skip that first step entirely by just copy-and-pasting this output into
our original profile. Let's measure things again now that we've made this change:

```powershell
& pwsh.exe -NoProfile -Command {Import-Module PSProfiler; Measure-Script $profile;}

# Count  Line       Time Taken Statement
# -----  ----       ---------- ---------
#     1     1    00:00.0790705 Import-Module PSReadLine;
#     1     2    00:00.0279149 Set-PSReadLineOption -EditMode Windows
#     1     3    00:00.0010231 Set-PSReadLineOption -PredictionSource HistoryAndPlugin
#     1     4    00:00.0011608 Set-PSReadLineOption -PredictionViewStyle InlineView
#     0     5    00:00.0000000
#     2     6    00:00.0694422 (@(& '%LOCALAPPDATA%/Programs/oh-my-posh init pwsh ...
#     1     7    00:00.0006244 Enable-PoshTransientPrompt
#     1     8    00:00.0005833 Enable-PoshLineError

& pwsh.exe -NoProfile -Command {Measure-Command { . $profile };}

# ...
# TotalMilliseconds : 463.3328
```

We've successfully brought our oh-my-posh invocation down an order of magnitude and shaved a couple hundred milliseconds
off of our profile initialization. But... we can do better. A cool feature of
[powerlevel10k](https://github.com/romkatv/powerlevel10k) is "instant prompt", which allows a prompt to show
immediately, even while your profile is still loading. Let's reproduce this behavior in pwsh.

```powershell
# In our $profile...
Import-Module PSReadLine;

function prompt {
  if (Test-Path variable:global:ompjob) {
    # snip
  }
  $global:ompjob = Start-Job {(@(& '%LOCALAPPDATA%/Programs/oh-my-posh/bin/oh-my-posh.exe' init pwsh --config='<some config path>' --print) -join "`n")};
  write-host -ForegroundColor Blue "Loading `$profile in the background..."
  Write-Host -ForegroundColor Green -NoNewline "  $($executionContext.SessionState.Path.CurrentLocation) ".replace($HOME, '~');
  Write-Host -ForegroundColor Red -NoNewline "ᓚᘏᗢ"
  return " ";
}
```

First, we create a new `prompt` function; unsurprisingly this is the function that pwsh calls to render your prompt. Our
custom `prompt` function will first check if there's a global variable named `ompjob` - this is going to be a background
job in which we execute oh-my-posh. The first time `prompt` runs this variable will be unset, and so our `if` will be
skipped, and I've snipped it for readability. We'll come back to it.

Now, if the variable is unset, we'll set it to a new background job that executes our `oh-my-posh` invocation, but
without the `Invoke-Expression`. This will let the job's output be the text printed by oh-my-posh that we can consume
with `Invoke-Expression` later. Finally, we print out a nice little prompt - not as fancy as `oh-my-posh`'s, but it'll
do, _and_ it'll display almost instantly.

Finally, let's fill in the `if`-block for when the global variable _is_ set (on the second invocation of `prompt`):

```powershell
Import-Module PSReadLine;

function prompt {
  if (Test-Path variable:global:ompjob) {
    Receive-Job -Wait -AutoRemoveJob -Job $global:ompjob | Invoke-Expression;
    Remove-Variable ompjob -Scope Global;
    Enable-PoshTransientPrompt
    Enable-PoshLineError

    Set-PSReadLineOption -EditMode Windows
    Set-PSReadLineOption -PredictionSource HistoryAndPlugin
    Set-PSReadLineOption -PredictionViewStyle InlineView

    [console]::InputEncoding = [console]::OutputEncoding = New-Object System.Text.UTF8Encoding
    return prompt;
  }
  # snip
}
```

First, we get the output from the `ompjob`, and we `Invoke-Expression` it. In doing so, `oh-my-posh` redefines our
`prompt` function. We'll then initialize some other settings within `PSReadLine` and `oh-my-posh`. Finally, we return
whatever `oh-my-posh` produces in its redefined `prompt` function, and we're done! Let's profile this as well:

```powershell
& pwsh.exe -NoProfile -Command {Import-Module PSProfiler; Measure-Script $profile;}

# Count  Line       Time Taken Statement
# -----  ----       ---------- ---------
#     1     1    00:00.0789758 Import-Module PSReadLine;
#     0     2    00:00.0000000
#     0     3    00:00.0000000 function prompt {
#     0     4    00:00.0000000   if (Test-Path variable:global:ompjob) {
#     0     5    00:00.0000000     Receive-Job -Wait -AutoRemoveJob -Job $global:ompjob | Invoke-Expression;
#     0     6    00:00.0000000     Remove-Variable ompjob -Scope Global;
#     0     7    00:00.0000000     Enable-PoshTransientPrompt
#     0     8    00:00.0000000     Enable-PoshLineError
#     0     9    00:00.0000000
#     0    10    00:00.0000000     Set-PSReadLineOption -EditMode Windows
#     0    11    00:00.0000000     Set-PSReadLineOption -PredictionSource HistoryAndPlugin
#     0    12    00:00.0000000     Set-PSReadLineOption -PredictionViewStyle InlineView
#     0    13    00:00.0000000
#     0    14    00:00.0000000     [console]::InputEncoding = [console]::OutputEncoding = New-Object System.Text.UTF8Encoding
#     0    15    00:00.0000000     return prompt;
#     0    16    00:00.0000000   }
#     0    17    00:00.0000000   $global:ompjob = Start-Job {(@(& '%LOCALAPPDATA%/Programs/oh-my-posh/bin/oh-my-posh.exe' init pwsh ...
#     0    18    00:00.0000000   write-host -ForegroundColor Blue "Loading `$profile in the background..."
#     0    19    00:00.0000000   Write-Host -ForegroundColor Green -NoNewline "  $($executionContext.SessionState.Path.CurrentLocation) ".replace($HOME, '~');
#     0    20    00:00.0000000   Write-Host -ForegroundColor Red -NoNewline "ᓚᘏᗢ"
#     0    21    00:00.0000000   return " ";
#     0    22    00:00.0000000 }

& pwsh.exe -NoProfile -Command {Measure-Command { . $profile };}

# ...
# TotalMilliseconds : 101.5553
```

Wow! We've almost completely eliminated the overhead of importing our profile, and pushed that execution time into the
background while a user is typing in their first prompt and digesting its output. I'd move the
`Import-Module PSReadLine` into the background as well, except that module doesn't import correctly when you do this.]]></content><author><name>Sidneys1</name></author><category term="programming" /><category term="programming" /><category term="powershell" /><summary type="html"><![CDATA[Recently I began using Oh My Posh for PowerShell 7+ (pwsh). One thing I noticed however is that it takes upward of a second to activate in my pwsh $profile. Let’s dig in and see if we can’t improve that.]]></summary></entry><entry><title type="html">How I Built This Website, And How to Do It Yourself</title><link href="https://sidneys1.github.io/programming/2022/08/22/how-I-built-this-website.html" rel="alternate" type="text/html" title="How I Built This Website, And How to Do It Yourself" /><published>2022-08-22T00:00:00-04:00</published><updated>2022-08-22T00:00:00-04:00</updated><id>https://sidneys1.github.io/programming/2022/08/22/how-I-built-this-website</id><content type="html" xml:base="https://sidneys1.github.io/programming/2022/08/22/how-I-built-this-website.html"><![CDATA[Sidneys1.com is built statically using [Jekyll](https://jekyllrb.com/), and then published on the world wide web, GitHub
Pages, Tor, and IPFS. How is this all accomplished, and how can you host your own website this way? Let's walk through
it step by step. We'll be looking into (over the course of several posts):

* Building a website with Jekyll
* Hosting on NearlyFreeSpeech.net
* Customizing the site layout and adding useful features
* Hosting on GitHub Pages
* Hosting on IPFS
* Hosting on Tor

<!--more-->

## Getting Started With Jekyll

I use a Windows PC, and I find that the Jekyll environment is easier to set up under the
[Windows Subsystem for Linux](https://docs.microsoft.com/en-us/windows/wsl/about), so that's what I'll be using going
forward in this post. First, we'll install Ruby via your package manager - Ubuntu 20.04 has Ruby 2.7 available on apt
via `apt install ruby`. Next we'll use the Ruby package manager, `gem` to install both Jekyll and Bundler:
`gem install bundler jekyll`. Finally, you can create a new Jekyll site with `jekyll new sitename`.

Once inside the new site folder, you can generate and serve your page with `bundle exec jekyll serve`! This will create
a development server on `http://localhost:4000`. Let's quickly talk about how Jekyll works:

1. You create Markdown or HTML files in your site's folder for each post you wish to make.
2. Jekyll processes these files (along with some templates - we'll get to these later) to generate static HTML files.
3. These static HTML files (in `_site`) are what you want to put on your web hosting! It's all static, so you don't need
   anything fancy like server-side PHP or client-side Angular or React.

To simplify the build-preview and build-release process, let's make a simple Makefile. GNU Make is a tool that allows
you to define simple build steps, and then figures out the interdependencies between these steps for you. Let's take
a look:

```make
all: build

build: _site/

serve:
	bundle exec jekyll serve --watch --livereload --force_polling --drafts --destination _site_live/

_site/:
	env JEKYLL_ENV=production bundle exec jekyll build

clean:
	rm -rf _site/
```

Let's break down each of these sections:
* `all: build`: "all" is the default rule run when you execute `make`. This line says when "all" is run, run "build"
  first.
* `build: _site/`: when the "build" rule is run, we depend on "_site/" being built first.
* `serve: ...`: If you execute `make serve`, this rule is triggered. Make will run the `bundle exec jekyll serve ...`
  command we've listed. The parameters included are:
  * `--watch`: live-rebuild any changes made to the posts as you edit them (very useful!).
  * `--livereload`: use LiveReload to automatically refresh your browser when changes are rebuilt.
  * `--force_polling`: this is a workaround for some WSL bugs. See
    [this WSL issue](https://github.com/Microsoft/BashOnWindows/issues/216).
  * `--drafts`: include posts under the `_drafts` folder (this is where you can put in-progress posts).
  * `--destination _site_live/`: this directs the `serve` command to build the static output under a separate folder
    than the normal `_site/` rule. This way we can't accidentally publish our live preview version of the website - if
    we did, it would be broken because all links would lead to `localhost` instead of your website's URL!
* `_site/: ...`: this rule matches a file path, `_site`, which is Jekyll's output folder. Make understands file paths
  and will know that this rule will build the contents of `_site`. Note that we use `env` to tell Jekyll to use the
  ["production" environment](https://jekyllrb.com/docs/configuration/environments/).

  We'll never run this rule manually (though you could with `make _site/`), but our `build` rule depends on this
  running, and so Make will automatically run it when needed.
* `clean: ...`: Run `make clean` to invoke this rule - in our case, we just delete the `_site/` folder.

## Modifying the Jekyll Configuration

Jekyll configuration is stored in the `_config.yml` file at the root of our site. There are a couple things in here
we'll want to adjust before we publish our site to the world wide web. First of course we'll want to set a few basics:

* `title`: The name of your site (shown in the header).
* `email`: A contact email address (shown in the footer).
* `url`: This one is important - this sets the url used when clicking on absolute links to other pages within your site!
* `baseurl`: Used in conjunction with `url`. The format used is `{url}{baseurl}/path/to/page.html`. If you're hosting
  nothing but Jekyll on your website, then leave this blank.

And that's about it! We'll dig into configuration more when we talk about hosting on Tor and IPFS.

## Hosting

Ok! Now that we have a (basic) site, how do we host it? Let's start simple and look at NearlyFreeSpeech.net, whom I've
been using for years. They're cheap, no-nonsense, and don't have any crazy hidden fees. They also have support for Let's
Encrypt certificates, making SSL support both easy and automatic. Head on over and set up an account. It'll walk you
through creating a site (your website hosting) and you can even register and connect the domain name all within their
system. Eventually you'll find the settings to be able to connect with SSH - enable this and let's update our Makefile
again. Add this rule to your Makefile:

```make
publish: _site/
	rsync --itemize-changes --checksum --recursive --compress --delete _site/* $USER@ssh.phx.nearlyfreespeech.net:.
```

Let's break it down again:
* `publish: _site/`: when we execute `make publish`, we want to make sure the rule to build the `_site/` folder runs
  first.
* `rsync [...] _site/* $USER@ssh.phx.nearlyfreespeech.net:.`: we'll use rsync (a remote file-copying tool) to connect to
  our hosting via SSH, synchronizing the contents of `_site` with the root (`.`) folder of our hosting. You'll want to
  replace `$USER` with your NearlyFreeSpeech.net username, of course.

  The options we're using are:
  * `--itemize-changes`: list changes as we go (for clarity).
  * `--checksum`: rely on a file checksum (not timestamps and file size) to determine if files need to be replaced.
  * `--recursive`: recurse into subdirectories of `_site/`.
  * `--compress`: HTML is very compressible, so why not save some bandwidth?
  * `--delete`: this allows rsync to remove files on your hosting that are no longer present in your `_site` folder.
    This can be useful if you rename or delete a file that you no longer want people to be able to access.

Running `make publish` now should build your website and then prompt your for your NearlyFreeSpeech.net password to
connect with SSH. Provide your password and watch as your site is made available!

## Up Next

In future posts in this series, we'll look at making some quality of life improvements to the default Jekyll layout and
theme, as well as hosting our site on IPFS!]]></content><author><name>Sidneys1</name></author><category term="programming" /><category term="programming" /><category term="meta" /><summary type="html"><![CDATA[Sidneys1.com is built statically using Jekyll, and then published on the world wide web, GitHub Pages, Tor, and IPFS. How is this all accomplished, and how can you host your own website this way? Let’s walk through it step by step. We’ll be looking into (over the course of several posts): Building a website with Jekyll Hosting on NearlyFreeSpeech.net Customizing the site layout and adding useful features Hosting on GitHub Pages Hosting on IPFS Hosting on Tor]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sidneys1.github.io/images/how-I-built-this-website/hero.png" /><media:content medium="image" url="https://sidneys1.github.io/images/how-I-built-this-website/hero.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Retro Roundup 2022</title><link href="https://sidneys1.github.io/retrocomputing/2022/06/03/retro-roundup.html" rel="alternate" type="text/html" title="Retro Roundup 2022" /><published>2022-06-03T15:26:00-04:00</published><updated>2022-06-03T15:26:00-04:00</updated><id>https://sidneys1.github.io/retrocomputing/2022/06/03/retro-roundup</id><content type="html" xml:base="https://sidneys1.github.io/retrocomputing/2022/06/03/retro-roundup.html"><![CDATA[Here's a quick roundup of the retrocomputing projects I worked on and devices I've acquired! Since this is the first
year I started collecting retro computers, it will also contain a few items that I've had for over a year as well.

<!--more-->

## Commodore 64

That's right! The O.G. home computer, and still the best-selling single model of computer to date. My particular model
is a brown breadbin.

### Commodore 1541 Disk Drive

Included with my C64 was a Commodore 1541 5.25" floppy disk drive. Not too much to say here.

### Games and Software

I also received a few 5.25" disks with the C64, however I can't locate them at this moment. I'll have to see if I can
find them and then update this post.

### Line Doubler

My initial attempts at connecting the C64 up to a modern television were not great - eventually I ordered a cheap line
doubler that accepted breakout RGB input and produced HDMI. I'm honestly unimpressed with this particular model (Amanka
AVI to HDMI) and instead ended up buying a S-Video cable for the C64 and using a CRT television instead - the difference
in picture quality is astounding.

### IEC2MicroSD

Another convenience purchase, this device allows me to use an SD card as if it were a floppy drive.

## Commodore VIC-20

A more recent purchase, I also acquired a Commodore VIC-20 (revision A or B board). You can see
[my entry on the VIC-20 registry](https://cbmvic.net/registry/569). This VIC currently powers on, but does not output a
video signal. I'm in the processes of figuring out what the problem is. It's in pretty rough shape, with a few damaged
parts on the case and missing the metallic Commodore VIC-20 badge (of which I've ordered a replacement).

This computer came in the original box, amazingly, and included the power adapter (actually soldered on to the
mainboard), several manuals, and the "Invaders" cartridge.

## Sun SPARCstation Haul

Wow - what can I say here? A few months ago I had a reply to a [FreeCycle](https://www.freecycle.org/) listing that I'd
posted in search of any early-90's to late-00's computers (this search also turned up the Dell CRT described below).
The person who messaged me had been a small businessowner in the early to mid 90's, and was looking to get rid of some
of their server room equipment that they'd been holding on to in a storage unit. I hastily agreed and a few months later
I went to pick up quite the haul:

* A WYSE CRT Serial Terminal
* A Nokia 447W CRT Monitor
* A Sun Microsystems PS/2 keyboard
* A Compaq Netelligent 100Base-T Class 1 Repeater
* A Dell PowerEdge 650 (this is from 2003, so much later in the company's lifetime than the majority of this equipment)
* An unlabeled beige PC tower (still need to look inside this one!)
* A Dell PowerEdge 1300 (also circa late 90's early 00's)
* An unknown rack-mount box with a "CTC" badge, locking front panel, and perhaps a LCD display (also need to crack this
  one open!)
* An Axil Ultima 1 (a third-party Sun SPARCstation Clone, see [this article][axil])
* A [Sun SPARCstation 20][ss20]
* A [Sun ULTRA 1 Creator][ultra1]

[axil]: https://fcw.com/1996/06/axil-targets-sun-with-sparc-compatible-graphics-computer/238823/
[ss20]: https://en.wikipedia.org/wiki/SPARCstation_20
[ultra1]: https://en.wikipedia.org/wiki/Ultra_1

## Compaq DeskPro Retro Battlestation

Another longer-term effort of mine has been to recreate my childhood computer - a Compaq DeskPro that sat on the desk
under my loft bed through most of my formative years. Of course, since we have the benefit of choice, I'm also
"upgrading" it into the ideal late-90's gaming computer!

The exact model I have is a later model DeskPro EN that is quite a bit smaller in overall form factor to my childhood
PC, however it does have the same general aesthetics that I was looking for. Unfortunately, so far I've been unable to
locate the matching Compaq keyboard and 800x600 CRT that I had as a kid as well... Though I did find a period-correct
Compaq PS/2 mouse!

### GeForce FX5200

I acquired a GeForce fx5200 PCI GPU for this computer - I originally had purchased a AGP 5200 by accident, so I suppose
I'll end up using that in some other computer down the line!

This card works quite well (once the appropriate DirectX 9 drivers are installed) in all of the games I've tested so
far.

### Microsoft Naturals Keyboard

A flea market find - a $5 Microsoft Naturals keyboard - a funky little beige PS/2 keyboard with an early PS/2&rarr;USB
adapter. In lieu of the Compaq keyboard I really want, this works in a pinch.

### Windows 98 Second Edition *(Third Edition?)*

Finally, this computer runs Windows 98 SE with many components of the unofficial "third edition" service pack installed
to add some much-needed quality-of-life features, including USB mass storage device support.

### Dell Monitor

Not much to say here, another FreeCycle find - while it does support a nice 1024x768 resolution, it has a mid-00's black
plastic aesthetic that I am not overly fond of.

## Compaq Presario 700

An EBay purchase, this is a early-00's laptop designed for use with Windows XP. Being from GoodWill it came without an
internal hard drive, which I've since replaced. Interestingly, it includes an expansion slot, a 3.5" floppy drive, *and*
a DVD-ROM and CD R-RW drive.

## Compaq Presario 1245

Another EBay purchase, this time a slightly older aesthetic laptop with a CD-ROM only drive and a floppy drive.

## Dell Inspiron 1420

This is my childhood laptop! Which now probably counts as "retro". Not much to say here, there are a few missing keys
because I once decided to try the DVORAK layout (which I got quite good at as I remember), but as it turns out Dell
in its infinite wisdom made the indexing keys (F and J) use a reversed butterfly hinge from the rest of the keys,
meaning those didn't *quite* fit in the alternate layout an I had to physically cut them to make them fit. Eventually
this physical damage led to them no attaching to the hinge as securely as they should have, and with time a few keys
became lost entirely.]]></content><author><name>Sidneys1</name></author><category term="retrocomputing" /><category term="retrocomputing" /><category term="collecting" /><summary type="html"><![CDATA[Here’s a quick roundup of the retrocomputing projects I worked on and devices I’ve acquired! Since this is the first year I started collecting retro computers, it will also contain a few items that I’ve had for over a year as well.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sidneys1.github.io/images/2022-06-03-retro-roundup/hero.png" /><media:content medium="image" url="https://sidneys1.github.io/images/2022-06-03-retro-roundup/hero.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Raytracing!</title><link href="https://sidneys1.github.io/programming/2022/03/23/raytracing.html" rel="alternate" type="text/html" title="Raytracing!" /><published>2022-03-23T13:04:39-04:00</published><updated>2022-03-23T13:04:39-04:00</updated><id>https://sidneys1.github.io/programming/2022/03/23/raytracing</id><content type="html" xml:base="https://sidneys1.github.io/programming/2022/03/23/raytracing.html"><![CDATA[Since I started programming I've had a dream in the back of my mind: *raytracers are super cool*, and I'd like to build
one myself. But with that dream accompanied another thought: *raytracers are nearly a pure expression of math*, a
discipline I am poorly qualified for.

However this winter I discovered a new programming community, [OneLoneCoder][OLC], and its leader, [javidx9][javid].
Watching the videos produced by javidx9 inspired me to take a leap of faith in myself and start this raytracing project.
The result has been amazing to see unfold as I developed first a working prototype in C#, then in C++, and finally as I
produced what hopefully is an easy to follow "tutorial" style Git repository. So, lets dive in!

<!--more-->

![finished product]({{"/images/2022-03-23-raytracing/finished.gif" | absolute_url}})

## What is Raytracing?

Raytracing is a method of rendering three dimensional scenes that is inspired by the physics behind light and vision. In
the real world, light emanates from some source and bounces off of objects before entering our eye and being processed
by the brain. In raytracing, however, this process is reversed, and a "ray" is sent out from the virtual camera into a
scene, collecting information about the objects it encounters, eventually returning a resolved color for that pixel of
the canvas.

## How Do We Begin?

### Creating a new `olc::PixelGameEngine` Project

We're going to start with <kbd>Create a new project</kbd> in Visual Studio (I'm using 2022). Select the <kbd>Empty
Project</kbd> (C++/Windows/Console) template. I also opted for the flat directory structure option <kbd>☑ Place solution
and project in the same directory</kbd>.

![Create new project]({{"/images/2022-03-23-raytracing/create-a-new-project.png" | absolute_url}})

We copy in the [`olcPixelGameEngine.h` file][olc-pge-header] and add it to our solution. We also add a blank `main.cpp`
and populate it with the contents of the template available in the `olcPixelGameEngine.h` header, taking care to rename
our game class to match our needs.

<div class="note" markdown=1>
<div class="note-title">Note</div>
Running our project will render a default PixelGameEngine scene: a 256x240 canvas of random pixels, magnified 4x:

{% capture pixel_barf %}{{ "/images/2022-03-23-raytracing/01-Add PGE header and create a game from template.png" | absolute_url }}{% endcapture %}
{% capture pixel_barf_thumb %}{{ "/images/2022-03-23-raytracing/thumbs/th-01-Add PGE header and create a game from template.png" | absolute_url }}{% endcapture %}
{% include popimg.html src=pixel_barf thumb=pixel_barf_thumb alt="Pixel Barf" %}

</div>

## Setting the Scene

### Add basic Shapes and a vector of shapes to render

We create a base class `Shape` and derived class `Sphere` (blank for now) that we will use to define our renderable
objects in the future.

We also add a `std::vector` of `std::unique_ptr<Shape>` to our game class. This will allow us to add new `Shape`-derived
objects to our scene:

```cpp
shapes.emplace_back(std::make_unique<Sphere>());
```

When the game exits, the memory we allocated will be freed (thanks, smart pointers).

### Add constants and a way to "Sample" single pixels

We define a few constants for window geometry and begin implementing our rendering process by looping over the rows and
columns of the scene and calling a `Sample` function that takes a floating-point x,y position on the viewport and
returns a `olc::Pixel` for that location.

```cpp
// Game width and height (in pixels).
constexpr int WIDTH = 250;
constexpr int HEIGHT = 250;

// Half the game width and height (to identify the center of the screen).
constexpr float HALF_WIDTH = WIDTH / 2.0f;
constexpr float HALF_HEIGHT = HEIGHT / 2.0f;
```

```cpp
bool OnUserUpdate(float fElapsedTime) override {
    // Called once per frame

    // Iterate over the rows and columns of the scene
    for (int y = 0; y < HEIGHT; y++) {
        for (int x = 0; x < WIDTH; x++) {
            // Sample this specific pixel (converting screen coordinates to scene coordinates).
            auto color = Sample(x - HALF_WIDTH, y - HALF_HEIGHT);
            Draw(x, y, color);
        }
    }

    return true;
}

olc::Pixel Sample(float x, float y) const {
    // Called to get the color of a specific point on the screen.
    // For now we're returning a color based on the screen coordinates.
    return olc::Pixel(std::abs(x * 255), std::abs(y * 255), 0);
}
```

<div class="note" markdown=1>
<div class="note-title">Note</div>
Running our project will now render a 250x250 canvas at 2x magnification. Our magenta fill has been replaced with a
color pattern converging in the center of the canvas:
{% capture coordinate_identity %}{{ "/images/2022-03-23-raytracing/03-Add constants and a way to Sample single pixels.png" | absolute_url }}{% endcapture %}
{% capture coordinate_identity_thumb %}{{ "/images/2022-03-23-raytracing/thumbs/th-03-Add constants and a way to Sample single pixels.png" | absolute_url }}{% endcapture %}
{% include popimg.html src=coordinate_identity thumb=coordinate_identity_thumb alt="Coordinate Identity" %}

</div>

### Add some geometry types, enhance Shape and Sphere

We add `struct`s for vectors (`vf3d`) and rays (`ray`). A vector represents a 3-dimensional point in space or a
3-dimensional magnitude, while a ray uses two vectors, one to represent an origin point, and one to represent a
directional magnitude out from that point.

```cpp
// Struct to describe a 3D floating point vector.
struct vf3d {
    float x, y, z;
    // Default constructor.
    vf3d() = default;
    // Explicit constructor that initializes x, y, and z.
    constexpr vf3d(float x, float y, float z) : x(x), y(y), z(z) {}
    // Explicit constructor that initializes x, y, and z to the same value.
    constexpr vf3d(float f) : x(f), y(f), z(f) {}
};

// Struct to describe a 3D floating point ray (vector with origin point).
struct ray {
    vf3d origin, direction;
    // Default constructor.
    ray() = default;
    // Add explicit constructor that initializes origin and direction.
    constexpr ray(const vf3d origin, const vf3d direction) : origin(origin), direction(direction) {}
};
```

We'll also enhance our `Shape` class with properties that will allow us to describe the size, position, and color of
shapes in our scene:

```cpp
// Class to describe any kind of object we want to add to our scene.
class Shape {
public:
    vf3d origin;
    olc::Pixel fill;
    // Delete the default constructor (we'll never have a Shape with a default origin and fill).
    Shape() = delete;
    // Add explicit constructor that initializes origin and fill.
    Shape(vf3d origin, olc::Pixel fill) : origin(origin), fill(fill) {}
};
```

We'll also give the `Sphere` class a notion of a `radius`, which is not shared by all `Shape`-derived classes:

```cpp
class Sphere : public Shape {
public:
    float radius;
    // Delete the default constructor (see "Shape() = delete;").
    Sphere() = delete;
    // Add explicit constructor that initializes Shape::origin, Shape::fill, and Sphere::radius.
    Sphere(vf3d origin, olc::Pixel fill, float radius) : Shape(origin, fill), radius(radius) {}
};
```

Next, we'll want to update our `OnUserCreate` function to pass in the newly required properties of a `Sphere`. Let's
create our initial `Sphere` at the position `x=0, y=0, z=200`, where $$x$$ is lateral position, $$y$$ is vertical
position, and $$z$$ is depth (or distance from the camera). Since our camera will be at `x=0, y=0, z=0` this will
align our `Sphere` in the center of the canvas, 200 units away. We'll also give our `Sphere` a solid `olc::GREY` color,
and set the `radius` to `100`.

```diff
- shapes.emplace_back(std::make_unique<Sphere>());
+ shapes.emplace_back(std::make_unique<Sphere>(vf3d(0, 0, 200), olc::GREY, 100));
```

Finally, using our newly created `ray` type, let's construct a ray in `Sample` for a given pixel that will point into
the scene:

```cpp
// Create a ray casting into the scene from this "pixel".
ray sample_ray({ x, y, 0 }, { 0, 0, 1 });
// TODO: We now need to test if this ray hits any Shapes and produces
//       a color.
```

We create the `ray` at origin `x=x, y=y, z=0`, and set the direction to `x=0, y=0, z=1`. The direction is what we call a
*unit vector*, which means that the overall magnitude to the vector is 1 ($$\sqrt{x^2+y^2+z^2}=1$$). Using a unit vector
will simplify some math for us later.

### Add fog color and a way to sample rays

To prevent our scene from extending into infinity, and to have something to show when a ray doesn't hit *anything*, we
add a new constant: a "fog" color.

```cpp
// A color representing scene fog.
olc::Pixel FOG(128, 128, 128);
```

Additionally, we add a more specific function, `SampleRay`, that is called by `Sample` to return the color (or absence
thereof) of a ray as it extends into our scene. For now, still, this returns a color relative to the $$x$$ and $$y$$
coordinate in our scene:

```cpp
// Add a new include at the top of our file:
#include <optional>

// ---✂---

std::optional<olc::Pixel> SampleRay(const ray r) const {
    // Called to get the color produced by a specific ray.
    // This will be the color we (eventually) return.
    olc::Pixel final_color;
    // For now we're returning a color based on the screen coordinates.
    return olc::Pixel(std::abs(x * 255), std::abs(y * 255), 0);
    final_color = olc::Pixel(std::abs(r.origin.x * 255), std::abs(r.origin.y * 255), 0);
    return final_color;
}
```

Don't forget to update `Sample` accordingly:

```diff
  // Create a ray casting into the scene from this "pixel".
  ray sample_ray({ x, y, 0 }, { 0, 0, 1 });
- // TODO: We now need to test if this ray hits any Shapes and produces
- //       a color.
+ // Sample this ray - if the ray doesn't hit anything, use the color of
+ // the surrounding fog.
+ return SampleRay(sample_ray).value_or(FOG);
```

### Add intersection and sample methods to Shapes

Our `SampleRay` function has been upgraded to search for a `Shape` that it intersects with. To do this, `Shape` has been
upgraded with two new virtual methods:

```cpp
// Get the color of this Shape (when intersecting with a given ray).
virtual olc::Pixel sample(ray sample_ray) const { return fill; }

// Determine how far along a given ray this Shape intersects (if at all).
virtual std::optional<float> intersection(ray r) const = 0;
```

These methods provide the ability to determine where along a `ray` a `Shape` intersects, and to provide the color of the
`Shape` at a given `ray` intersection. Our `Sphere` class overrides the `intersection` method, though for now
the implementation only returns an empty optional.

```cpp
// Determine how far along a given ray this Circle intersects (if at all).
std::optional<float> intersection(ray r) const override {
    // TODO: Implement ray-sphere intersection.
    return {};
}
```

Finally, let's update our `Sample` function, replacing the `final_color` value we compute based on screen coordinates
with an algorithm that searches a `Shape` that intersects with our given `ray`:

```cpp
// Store a pointer to the Shape this ray intersects with.
auto intersected_shape_iterator = shapes.end();
// Also store the distance along the ray that the intersection occurs.
float intersection_distance = INFINITY;

/* Determine the Shape this ray intersects with(if any). */ {
    // Iterate over all of the Shapes in our scene.
    for (auto it = shapes.begin(); it != shapes.end(); it++) {
        // If the distance is not undefined (meaning no intersection)...
        if (std::optional<float> distance = (*it)->intersection(r)) {
            // Save the current Shape as the intersected Shape!
            intersected_shape_iterator = it;
            // Also save the distance along the ray that this intersection occurred.
            intersection_distance = distance.value();
        }
    }
    // If we didn't intersect with any Shapes, return an empty optional.
    if (intersected_shape_iterator == shapes.end())
        return {};
}
// Get the shape we discovered
const Shape &intersected_shape = **intersected_shape_iterator;
// Set our color to the sampled color of the Shape this ray with.
final_color = intersected_shape.sample(r);
```

## Rendering Shapes

### Implement ray-Sphere intersection

We'll need to overload some operators for a `vf3d`: subtraction, and dot-product. A dot-product is a useful way of
comparing two vectors to determine if they are similar. Consider two vectors, $$a$$ and $$b$$. The dot-product is a
scalar value determined like so:

$$dot(a,b) = a\cdot{}b = a_{1}b_{1}+a_{2}b_{2}+a_{3}b_{3}$$

So, for example:

$$\begin{bmatrix} 1 & 2 & 3\end{bmatrix}\cdot\begin{bmatrix} 4 & 5 & 6\end{bmatrix}\newline=\newline(1\cdot4)+(2\cdot5)+(3\cdot6)\newline=\newline4+10+18\newline=\newline32$$

Translating this to C++ gives us our dot product function:

```cpp
// Dot product (multiplication): vf3d * vf3d = float
const float operator* (const vf3d right) const {
    return (x * right.x) + (y * right.y) + (z * right.z);
}

// Subtraction: vf3d - vf3d = vf3d
const vf3d operator-(const vf3d right) const {
    return { x - right.x, y - right.y, z - right.z };
}
```

We'll also implement the equation for an intersection between a `ray` and a `Sphere`. I'm not going to go into depth
explaining the geometry here: this is a well-documented process and can be researched separately.

```cpp
std::optional<float> intersection(ray r) const override {
    return {};
    vf3d oc = r.origin - origin;
    float a = r.direction * r.direction;
    float b = 2.0f * (oc * r.direction);
    float c = (oc * oc) - (radius * radius);
    float discriminant = powf(b, 2) - 4 * a * c;
    if (discriminant < 0)
        return {};
    auto ret = (-b - sqrtf(discriminant)) / (2.0f * a);
    if (ret < 0)
        return {};
    return ret;
}
```

<div class="note" markdown=1>
<div class="note-title">Note</div>
Running our project will now render a (highly aliased and flatly-colored) Sphere!
{% capture flat_sphere %}{{ "/images/2022-03-23-raytracing/07-Implement ray-Sphere intersection.png" | absolute_url }}{% endcapture %}
{% capture flat_sphere_thumb %}{{ "/images/2022-03-23-raytracing/thumbs/th-07-Implement ray-Sphere intersection.png" | absolute_url }}{% endcapture %}
{% include popimg.html src=flat_sphere thumb=flat_sphere_thumb alt="Flat Sphere" %}

</div>

### Add perspective rendering and depth sorting

First we'll add some additional Spheres to our scene at different Z-depths. If we run our project now, you'll see that
the Spheres added to our scene last are drawn in front of the earlier ones, even if they are further away.

```cpp
// Add some additional Spheres at different positions.
shapes.emplace_back(std::make_unique<Sphere>(vf3d(-150, +75, +300), olc::RED, 100));
shapes.emplace_back(std::make_unique<Sphere>(vf3d(+150, -75, +100), olc::GREEN, 100));
```

![Bad Z-depth]({{"/images/2022-03-23-raytracing/bad-z.png" | absolute_url}})

To remedy that, we up date our hit check in `SampleRay` to select the object whose intersection is nearest to the `ray`
origin. Now if we run our project, the Spheres are properly sorted.

```diff
- if (std::optional<float> distance = (*it)->intersection(r)) {
+ if (std::optional<float> distance = (*it)->intersection(r).value_or(INFINITY);
+                 distance < intersection_distance) {
```

![no-perspective]({{"/images/2022-03-23-raytracing/no-perspective.png" | absolute_url}})

However, you'll notice that all three Spheres are the same size, despite being different distances from the camera. To
fix *this*, we'll need to add perspective to our camera. We'll do this in a very simplistic manner, by having all of the
rays originate from some point, and pointing towards what you can think of as a "virtual canvas" some distance in front
of that origin point. Update `Sample` like so:

```diff
- ray sample_ray({ x, y, 0 }, { 0, 0, 1 });
+ ray sample_ray({ 0, 0, -800 }, { (x / (float)WIDTH) * 100, (y / (float)HEIGHT) * 100, 200});

  // Sample this ray - if the ray doesn't hit anything, use the color of
  // the surrounding fog.
- return SampleRay(sample_ray).value_or(FOG);
+ return SampleRay(sample_ray.normalize()).value_or(FOG);
```

Notice we call method of `ray` that we haven't defined yet: `normalize()`. Normalization produces a normalized vector
(discussed before) from a non-normalized vector by resizing the vector components in proportion to their length such
that the overall length is still 1. Normalization is defined as (with $$v\cdot{}v$$ of course being the dot product of
itself):

$$normalize(v) = \frac{v}{\sqrt{v\cdot{}v}}$$

Let's add `normalize()` to both `ray` and `vf3d`:

```cpp
// ---✂--- In ray:

// Return a normalized version of this ray (magnitude == 1).
const ray normalize() const {
    return { origin, direction.normalize() };
}

// ---✂--- In vf3d:

// Division: vf3d / float = vf3d
const vf3d operator/(float divisor) const {
    return { x / divisor, y / divisor, z / divisor };
}

// Return a normalized version of this vf3d (magnitude == 1).
const vf3d normalize() const {
    return (*this) / sqrtf((*this) * (*this));
}
```

By normalizing this ray we get rays properly fanning out in a perspective.

<div class="note note-wide" markdown=1>
<div class="note-title">Note</div>
Running our project will now produce a proper perspective rendering of our three flat-shaded Spheres, at the correct
depths.
{% capture perspective %}{{ "/images/2022-03-23-raytracing/08-Add perspective rendering and depth sorting.png" | absolute_url }}{% endcapture %}
{% capture perspective_thumb %}{{ "/images/2022-03-23-raytracing/thumbs/th-08-Add perspective rendering and depth sorting.png" | absolute_url }}{% endcapture %}
{% include popimg.html src=perspective thumb=perspective_thumb alt="Perspective Rendering" %}

<details>
<summary>Open Image Comparison</summary>
{% include imgcomp.html asrc=flat_sphere_thumb aalt="" bsrc=perspective_thumb balt="" width="471px" height="500px" %}
</details>

</div>

### Add a Plane Shape, and apply fog

First we'll add a new type of `Shape`: a `Plane`. This is a flat surface extending infinitely in all directions. I'm not
going to go into depth about the intersection algorithm, as that's basic geometry and is better explained elsewhere.
Unlike a `Sphere`, orientation matters to a `Plane`, so we'll also add a "direction" `vf3d` that will indicate the
normal pointing away from the surface.

```cpp
// Subclass of Shape that represents a flat Plane.
class Plane : public Shape {
public:
	vf3d direction;

	// Add explicit constructor that initializes
	Plane(vf3d origin, vf3d direction, olc::Pixel fill) : Shape(origin, fill), direction(direction) {}

	// Determine how far along a given ray this Plane intersects (if at all).
	std::optional<float> intersection(ray sample_ray) const override {
		auto denom = direction * sample_ray.direction;
		if (fabs(denom) > 0.001f) {
			auto ret = (origin - sample_ray.origin) * direction / denom;
			if (ret > 0) return ret;
		}
		return {};
	}
};
```

We will also override the `sample` virtual method for the first time to provide a checkerboard pattern that will make
the perspective rendering we added last time really pop.

```cpp
// Get the color of this Plane (when intersecting with a given ray).
// We're overriding this to provide a checkerboard pattern.
olc::Pixel sample(ray sample_ray) const override {
	// Get the point of intersection.
	auto intersect = (sample_ray * intersection(sample_ray).value_or(0.0f)).end();

	// Get the distances along the X and Z axis from the origin to the intersection.
	float diffX = origin.x - intersect.x;
	float diffZ = origin.z - intersect.z;

	// Get the XOR the signedness of the differences along X and Z.
	// This allows us to "invert" the +X,-Z and -X,+Z quadrants.
	bool color = (diffX < 0) ^ (diffZ < 0);

	// Flip the "color" boolean if diff % 100 < 50 (e.g., flip one half of each 100-unit span.
	if (fmod(fabs(diffZ), 100) < 50) color = !color;
	if (fmod(fabs(diffX), 100) < 50) color = !color;

	// If we're coloring this pixel, return the fill - otherwise return DARK_GREY.
	if (color)
		return fill;
	return olc::DARK_GREY;
}
```

To do this we'll also be adding some new operator overloads to both `vf3d` and `ray`, and we'll also add a new method to
`ray` that returns the `vf3d` representing the endpoint of the `ray`.

```cpp
// ---✂--- In vf3d:

// Addition: vf3d + vf3d = vf3d
const vf3d operator+(const vf3d right) const {
	return { x + right.x, y + right.y, z + right.z };
}

// Multiplication: vf3d * float = vf3d
const vf3d operator*(float factor) const {
	return { x * factor, y * factor, z * factor };
}

// ---✂--- In ray:

// Multiplication: ray * float = ray
const ray operator*(float right) const {
	return { origin, direction * right };
}

// Return the vf3d at the end of this ray.
const vf3d end() const {
	return origin + direction;
}
```

Finally, we'll add a new `Plane` to our scene just below our `Sphere`s. Note that since we render our canvas top to
bottom, +Y is down, while -Y is up.

```cpp
// Add a "floor" Plane
shapes.emplace_back(std::make_unique<Plane>(vf3d(0, 200, 0 ), vf3d(0, -1, 0), olc::Pixel(204, 204, 204)));
```

<div class="note" markdown=1>
<div class="note-title">Note</div>

Running our project now you'll see the checkerboard pattern continue off to the horizon - *however*, it appears
further up on the canvas than you might expect. *Additionally*, the checkerboard pattern gets very garbled as the
checks gets smaller than a single pixel, creating a sort of unexpected and disorienting moire pattern. Perhaps drawing
surfaces *that* far away isn't good...

*Coming soon: a screenshot.*
<!-- TODO: ![Plane.]() -->

</div>

To remedy this, we'll add the concept of Fog. We already have a Fog color, for when a ray doesn't hit anything. This new
concept applies the idea of there being some extreme translucency to the nothingness between a ray's origin and the
`Shape` it intersects with. We'll begin by adding two new constants, one to define the maximum distance at which an
`Shape` would be visible, and the other as the reciprocal of that.

```cpp
// Fog distance and reciprocal (falloff).
constexpr float FOG_INTENSITY_INVERSE = 3000;
constexpr float FOG_INTENSITY = 1 / FOG_INTENSITY_INVERSE;
```

Now when we're determining the color of a ray in `SampleRay` we can check if the intersection distance is greater than
that of the max Fog distance. If so, we'll immediately return the Fog color and skip further calculation. If the
distance is lower, however, we want to smoothly transition between the `Shape`'s color and the Fog's color, depending on
the distance.

```cpp
// Quick check - if the intersection is further away than the furthest Fog point,
// then we can save some time and not calculate anything further, since it would
// be obscured by Fog regardless.
if (intersection_distance >= FOG_INTENSITY_INVERSE)
	return FOG;

// Set our color to the sampled color of the Shape this ray with.
final_color = intersected_shape.sample(r);

// Apply Fog
if (FOG_INTENSITY)
	final_color = lerp(final_color, FOG, intersection_distance * FOG_INTENSITY);
```

To do this, we've referenced a function called `lerp` - short for "linear interpolation" - that we haven't defined yet.
This function smoothly mixes two colors based on a floating point value between 0.0 and 1.0.

```cpp
// Apply a linear interpolation between two colors:
//  from |-------------------------------| to
//                ^ by
olc::Pixel lerp(olc::Pixel from, olc::Pixel to, float by) const {
	if (by <= 0.0f) return from;
	if (by >= 1.0f) return to;
	return olc::Pixel(
		from.r * (1 - by) + to.r * by,
		from.g * (1 - by) + to.g * by,
		from.b * (1 - by) + to.b * by
	);
}
```

<div class="note note-wide" markdown=1>
<div class="note-title">Note</div>

Running our project now displays our Spheres as before, plus the checkerboard Plane of the floor, smoothly fading
into the distance.
{% capture floor %}{{ "/images/2022-03-23-raytracing/09-Add a Plane Shape, and apply Fog.png" | absolute_url }}{% endcapture %}
{% capture floor_thumb %}{{ "/images/2022-03-23-raytracing/thumbs/th-09-Add a Plane Shape, and apply Fog.png" | absolute_url }}{% endcapture %}
{% include popimg.html src=floor thumb=floor_thumb alt="Floor and Fog" %}

<details>
<summary>Open Image Comparison</summary>
{% include imgcomp.html asrc=perspective_thumb aalt="" bsrc=floor_thumb balt="" width="471px" height="500px" %}
</details>

</div>

Note that as our scene and renderer grow in complexity we'll begin to see lower and lower frame-rates when running our
project. Switching our compilation mode to Release and running without debugging can help, as the compiler will more
aggressively apply optimizations. Feel free to experiment with optimization strategies in the Release compilation
settings.

## Prettying Up

### Add reflections

Reflections are an intrinsic feature of any raytracer. To begin, let's add a new constant to control just how many times
a ray may reflect as it makes its way through the scene. Imagine being inside a hall of mirrors - the reflections may
continue to some recursive depth - in real life, this is infinite (or at least to the degree allowed by the quality of
the mirrors and available light). In our project, reflections add more computational complexity, so limiting the degree
to which these reflections propagate is essential. To do so, I've surrounded two different values for this constant in
preprocessor "if" statements to provide different values under Debug and Release mode respectively.

```cpp
#ifdef DEBUG
constexpr int BOUNCES = 2;
#else
constexpr int BOUNCES = 5;
#endif
```

Next we'll add a property to our base `Shape` class - a floating point representing `reflectivity`. This will range
between 0 (no reflections) and 1 (a perfect mirror). We'll also initialize this as a constructor parameter, and extend
that parameter to the `Sphere` class as well.

```cpp
// ---✂--- In Shape:

vf3d origin;
olc::Pixel fill;
float reflectivity;

/* CONSTRUCTORS */

// Delete the default constructor (we'll never have a Shape with a default origin and fill).
Shape() = delete;

// Add explicit constructor that initializes origin and fill.
Shape(vf3d origin, olc::Pixel fill, float reflectivity = 0.0f) : origin(origin), fill(fill), reflectivity(reflectivity) {}

// ---✂--- In Sphere:

Sphere(vf3d origin, olc::Pixel fill, float radius, float reflectivity = 0.0f) : Shape(origin, fill, reflectivity), radius(radius) {}
```

Next, we'll make the surfaces of our first and second `Sphere`s to be
reflective.

```diff
 // Create a new Sphere and add it to our scene.
-shapes.emplace_back(std::make_unique<Sphere>(vf3d(0, 0, 200), olc::GREY, 100));
+shapes.emplace_back(std::make_unique<Sphere>(vf3d(0, 0, 200), olc::GREY, 100, 0.9f));

 // Add some additional Spheres at different positions.
-shapes.emplace_back(std::make_unique<Sphere>(vf3d(-150, +75, +300), olc::RED, 100));
+shapes.emplace_back(std::make_unique<Sphere>(vf3d(-150, +75, +300), olc::RED, 100, 0.5f));
```

Lastly, we'll add a new abstract method to our `Shape` class that will return the normal at a given
intersection point, and override this abstract method in our `Sphere` and `Plane` classes. A normal is simply a ray
pointing outwards from the surface of the `Shape` at the given point.

```cpp
// ---✂--- In Shape:

// Determine the surface normal of this Shape at a given intersection point.
virtual ray normal(vf3d incident) const = 0;

// ---✂--- In Sphere:

// Return the surface normal of this Sphere at a given intersection point.
ray normal(vf3d incident) const override {
	return { incident, (incident - origin).normalize() };
}
```

Next, let's enhance our `SampleRay` method by adding a parameter for how many "bounces" are allowed - as this method is
called recursively we'll decrement this value, at at the point where bounces is zero we'll stop processing more
reflections. We'll pass the initial bounces constant into the `SampleRay` method from the `Sample` method.

```cpp
// ---✂--- In Sample():

// Sample this ray - if the ray doesn't hit anything, use the color of
// the surrounding fog.
return SampleRay(sample_ray.normalize(), BOUNCES).value_or(FOG);

// ---✂--- In SampleRay():

std::optional<olc::Pixel> SampleRay(ray r, int bounces) const {
	bounces--;
```

Once we've
sampled our `Shape` and determined its intrinsic color, we need to created a reflected ray and sample that to determine
the color that would be reflected by this `Shape` - we can skip this process if the reflectivity is zero or if we've
reached the max depth. Creating a reflected ray is a simple geometric function between the direction of the original
`sample_ray` and the `Shape`'s normal at the intersection point. Finally, we sample this new ray (passing in the new,
decremented bounces count). We'll mix our `final_color` between the intrinsic color of this `Shape` itself and the color
we sampled along the reflected ray (or, if it didn't hit anything, our Fog color).

```cpp
// Set our color to the sampled color of the Shape this ray with.
final_color = intersected_shape.sample(r);

// Determine the point at which our ray intersects our Shape.
vf3d intersection_point = (r * intersection_distance).end();
// Calculate the normal of the given Shape at that point.
ray normal = intersected_shape.normal(intersection_point);

// Apply reflection
if (bounces != 0 && intersected_shape.reflectivity > 0) {
	// Our reflection ray starts out as our normal...
	ray reflection = normal;

	// Apply a slight offset *along* the normal. This way our reflected ray will
	// start at some slight offset from the surface so that rounding errors don't
	// cause it to collide with the Shape it originated from!
	reflection.origin = reflection.origin + (normal.direction + 0.001f);

	// Reflect the direction around the normal with some simple geometry.
	reflection.direction = (normal.direction * (2 * ((r.direction * -1) * normal.direction)) + r.direction).normalize();

	// Recursion! Since SampleRay doesn't care if the ray is coming from the
	// canvas, we can use it to get the color that will be reflected by this Shape!
	std::optional<olc::Pixel> reflected_color = SampleRay(reflection, bounces);

	// Finally, mix our Shape's color with the reflected color (or Fog color, in case
	// of a miss) according to the reflectivity.
	final_color = lerp(final_color, reflected_color.value_or(FOG), intersected_shape.reflectivity);
}
```

<div class="note note-wide" markdown=1>
<div class="note-title">Note</div>

Running our project at this point produces a beautifully rendered scene where the center and left `Sphere`s reflect
their surroundings - and a sharp eye can determine that the left `Sphere` can even see itself in its reflection of the
center `Sphere`.
{% capture reflect %}{{ "/images/2022-03-23-raytracing/10-Add reflections.png" | absolute_url }}{% endcapture %}
{% capture reflect_thumb %}{{ "/images/2022-03-23-raytracing/thumbs/th-10-Add reflections.png" | absolute_url }}{% endcapture %}
{% include popimg.html src=reflect thumb=reflect_thumb alt="Reflections" %}

<details>
<summary>Open Image Comparison</summary>
{% include imgcomp.html asrc=floor_thumb aalt="" bsrc=reflect_thumb balt="" width="471px" height="500px" %}
</details>

</div>

To further highlight the reflections we'll add some simple motion to the scene by accumulating time in the
`OnUserUpdate` function, and modifying the Y and Z coordinates of the center `Sphere` along a sine/cosine wave
respectively.

```cpp
// Called once per frame

// Create some static storage to accumulate elapsed time...
static float accumulated_time = 0.0f;

// ...and accumulate elapsed time into it.
accumulated_time += fElapsedTime;

// Update the position of our first Circle every update.
// sin/cos = easy, cheap motion.
Shape& shape = *shapes.at(0);
shape.origin.y = sinf(accumulated_time) * 100 - 100;
shape.origin.z = cosf(accumulated_time) * 100 + 100;

// Iterate over the rows and columns of the scene
```

<div class="note" markdown=1>
<div class="note-title">Note</div>

Running our project now will display a smoothly floating `Sphere`, with appropriate reflections of its surrounding
`Shapes`.

</div>

### Create and use a `color3` type

To simplify some upcoming features, let's replace our use of `olc::Pixel` with our own color type. Since we're used to
representing color as the combination of three values - red, green, and blue - we can represent each with a floating
point value between 0 and 1.

Looking at our code, we already have a type to represent three floating point values: `vf3d`. Using a simple `using`
alias, we can create a type alias called `color3` that is really a `vf3d` behind the scenes. This aliasing isn't really
necessary, but it will help avoid confusion. Additionally, we can leverage the `olc::PixelF` type to convert our
floating point color to one compatible with PixelGameEngine.

```cpp
// Use a type alias to use vf3d and color3 interchangeably.
using color3 = vf3d;

// ---✂---

// Colors

color3 LIGHT_GRAY(0.8f);
color3 DARK_GRAY(0.5f);
color3 GREY(0.75f);
color3 RED(1.0f, 0.0f, 0.0f);
color3 GREEN(0.0f, 1.0f, 0.0f);

// ---✂---

color3 FOG = DARK_GRAY;

// ---✂---

Draw(x, y, olc::PixelF(color.x, color.y, color.z));
```

You'll need to update references to `olc::Pixel` throughout the code to use `color3` instead. For example in our `lerp`
function:

```diff
 // Apply a linear interpolation between two colors:
 //  from |-------------------------------| to
 //                ^ by
-olc::Pixel lerp(olc::Pixel from, olc::Pixel to, float by) const {
+color3 lerp(color3 from, color3 to, float by) const {
 	if (by <= 0.0f) return from;
 	if (by >= 1.0f) return to;
-	return olc::Pixel(
-		from.r * (1 - by) + to.r * by,
-		from.g * (1 - by) + to.g * by,
-		from.b * (1 - by) + to.b * by
+	return color3(
+		from.x * (1 - by) + to.x * by,
+		from.y * (1 - by) + to.y * by,
+		from.z * (1 - by) + to.z * by
 	);
 }
```

<div class="note" markdown=1>
<div class="note-title">Note</div>

Running our project now produces no difference from our previous commit.

</div>

### Add diffuse lighting

Let's add a single point light source to our scene. We'll add a member to our game class to represent this. We'll use a
class member instead of a constant so that we can change the position of the light later. We'll initialize this value in
the constructor to be 500 units behind and 500 units above our origin.

```diff
 // ---✂--- Add a class member to our OlPixelRayTracer:

+// The position of our point light.
+vf3d light_point;

 // ---✂--- Update our game constructor:

-OlcPixelRayTracer() {
+OlcPixelRayTracer() : light_point(0, -500, -500) {
```

Diffuse lighting is frighteningly simple - we already know that a dot product between two vectors returns a value that
roughly describes the similarity of the vectors. To implement simple diffuse lighting, we can multiply our sample color
by a dot product between the surface normal vector and a vector pointing towards our single light source.

Let's add a section to our `SampleRay` function after we apply reflections where we'll apply diffuse lighting. The
process only requires three lines of code! First we'll create a normalized ray at the intersection point, pointing
towards the light point (we do this by subtracting the light point from the intersection point). Secondly, we'll
calculate the dot product between our light ray and the surface normal we already have.

```cpp
// ---✂--- After applying reflections, and before applying fog:

// Apply diffuse lighting

// First we'll get the normalized ray from our intersection point to the light source.
ray light_ray = ray(intersection_point, light_point - intersection_point).normalize();

// Next we'll compute the dot product between our surface normal and the light ray.
float dot = light_ray.direction * normal.direction;

// Multiplying our final color by this dot product darkens surfaces pointing away from the light.
final_color = final_color * dot;
```

<div class="note" markdown=1>
<div class="note-title">Note</div>

Running our project now will highlight a problem: the top halves of our `Shape`s look correct (towards the light), but
the bottoms have a corrupted look. You'll remember that the dot product of two vectors lies in the range $$[-1,1]$$. As we
reach the side of our `Shape`s pointing away from the light, our dot product enters the negative range - and "negative"
colors are certainly a concept our data types are unprepared to handle! To fix this let's clamp the dot product value to
the range $$[0,1]$$ - this way all negative values are discarded.


*Coming soon: a screenshot.*
<!-- TODO: ![Negative colors.]() -->

</div>

```diff
 // Next we'll compute the dot product between our surface normal and the light ray.
+// We need to clamp this between 0 and 1, because negative values have no meaning here.
-float dot = light_ray.direction * normal.direction;
+float dot = std::clamp(light_ray.direction * normal.direction, 0.0f, 1.0f);
```

<div class="note" markdown=1>
<div class="note-title">Note</div>

Running our project now looks correct! The tops of our `Shape`s are light, while the bottoms are almost pitch black.
**However**, since darkness isn't terribly interesting, let's add a global ambient light, which we'll implement as a new
constant.


*Coming soon: a screenshot.*
<!-- TODO: ![Too dark.]() -->

</div>

By adding our global light value to the dot product we'll ensure that our diffuse lighting never completely
darkens our scene.

```diff
 // ---✂--- Add a new constant:

+// Lighting
+constexpr float AMBIENT_LIGHT = 0.5f;

 // ---✂--- Update our diffuse lighting:

 // Next we'll compute the dot product between our surface normal and the light ray.
 // We need to clamp this between 0 and 1, because negative values have no meaning here.
+// Additionally, we'll add in our ambient light so no surfaces are entirely dark.
-float dot = std::clamp(light_ray.direction * normal.direction, 0.0f, 1.0f);
+float dot = std::clamp(AMBIENT_LIGHT + (light_ray.direction * normal.direction), 0.0f, 1.0f);
```

<div class="note note-wide" markdown=1>
<div class="note-title">Note</div>

Running our project now displays simple diffuse lighting without darkening any parts of our scene entirely.
{% capture diffuse %}{{ "/images/2022-03-23-raytracing/12-Add diffuse lighting.png" | absolute_url }}{% endcapture %}
{% capture diffuse_thumb %}{{ "/images/2022-03-23-raytracing/thumbs/th-12-Add diffuse lighting.png" | absolute_url }}{% endcapture %}
{% include popimg.html src=diffuse thumb=diffuse_thumb alt="Diffuse Lighting" %}

<details>
<summary>Open Image Comparison</summary>
{% include imgcomp.html asrc=reflect_thumb aalt="" bsrc=diffuse_thumb balt="" width="471px" height="500px" %}
</details>

</div>

### Add shadow casting

Let's upgrade our lighting mechanic with proper shadows. The theory is simple: we check if any `Shape`s intersect with
the ray between a `Shape`'s surface and the light itself. If any `Shape`s do intersect, then the light is fully occluded
and we can set the lighting to full-dark (rather than the diffuse value we calculated last time). If there are no
intersecting `Shape`s, then we use the dot product as we did last time.

To start, instead of normalizing our lighting ray immediately, we'll want to save its length - this lets us know how far
away the light is from the current hit point. To do this we'll add a short method to `vf3d` to calculate its length.

```cpp
// Return the length of this vf3d.
const float length() const {
	return sqrtf(x * x + y * y + z * z);
}
```

Next, to avoid the lighting ray intersecting with the current object itself we'll offset the light ray origin by a tiny
amount along the surface normal. Finally, we'll normalize the light ray's direction.

```cpp
// ---✂--- In SampleRay(), replace our current diffuse lighting with the following:

// Apply lighting

// First we'll get the un-normalized ray from our intersection point to the light source.
ray light_ray = ray(intersection_point, light_point - intersection_point);
// Get the distance to the light (equal to the length of the un-normalized ray).
float light_distance = light_ray.direction.length();
// We'll also offset the origin of the light ray by a small amount along the
// surface normal so the ray doesn't intersect with this Shape itself.
light_ray.origin = light_ray.origin + (normal.direction * 0.001f);
// And finally we'll normalize the light_ray.
light_ray.direction = light_ray.direction.normalize();
```

To determine if any `Shape`s intersect with this ray, we'll use a simplified version of our search loop from before -
however this time we don't care which `Shape` is intersecting, just whether one has. Additionally, we don't care about
`Shape`s that intersect with the ray that are further from the origin than the light itself (meaning that `Shape` is on
the far side of the light), so we'll initialize our search distance to the distance to the light itself.

```cpp
// Then we'll search for any Shapes that is occluding the light_ray,
// using more or less our existing search code.
// We initialize closest_distance to our light distance, because we
// don't care if any of the Shapes intersect the ray beyond the light.
float closest_distance = light_distance;
for (auto& shape : shapes) {
	if (float distance = shape->intersection(light_ray).value_or(INFINITY); distance < closest_distance) {
		closest_distance = distance;
	}
}
```

Finally, if the final search distance is less than the distance to the light itself then we have a `Shape` occluding the
ray! In this case we can skip the dot-product diffuse calculation and just multiply the color by our ambient light.
Otherwise we calculate the diffuse lighting as before.

```cpp
// Check if we had an intersection (the light is occluded).
if (closest_distance < light_distance) {
	// Multiplying our final color by the ambient light darkens this surface "entirely".
	final_color = final_color * AMBIENT_LIGHT;
}  else {
	// Next we'll compute the dot product between our surface normal and the light ray.
	// We need to clamp this between 0 and 1, because negative values have no meaning here.
	// Additionally, we'll add in our ambient light so no surfaces are entirely dark.
	float dot = std::clamp(AMBIENT_LIGHT + (light_ray.direction * normal.direction), 0.0f, 1.0f);

	// Multiplying our final color by this dot product darkens surfaces pointing away from the light.
	final_color = final_color * dot;
}
```

<div class="note note-wide" markdown=1>
<div class="note-title">Note</div>

Running our project now will render shadows cast upon other `Shape`s in the scene that dynamically update as the
`Shape`s or light itself move.
{% capture shadow %}{{ "/images/2022-03-23-raytracing/13-Add shadow casting.png" | absolute_url }}{% endcapture %}
{% capture shadow_thumb %}{{ "/images/2022-03-23-raytracing/thumbs/th-13-Add shadow casting.png" | absolute_url }}{% endcapture %}
{% include popimg.html src=shadow thumb=shadow_thumb alt="Shadow Casting" %}

<details>
<summary>Open Image Comparison</summary>
{% include imgcomp.html asrc=diffuse_thumb aalt="" bsrc=shadow_thumb balt="" width="471px" height="500px" %}
</details>

</div>

### Add multisampling

One very noticeable shortcoming of our current renderer is the strong aliasing - since we always cast our ray towards
the exact center of every pixel, we don't get any sort of antialiasing effect for pixels that are only partially covered
by a given `Shape` or feature such as shadows or reflections.

An easy antialiasing solution is to implement multisampling, which is the process of sending multiple rays into each
pixel and averaging the results. By varying the angle of each ray slightly we can average out aliasing error.

Let's add a constant to define how many samples we'll take for each pixel.

```cpp
constexpr int SAMPLES = 3;
```

Then, as we iterate over the rows and columns of our canvas, let's instead create an array of colors the same size as
our number of samples, and for each index in this array we'll generate a random offset in the X and Y dimensions, and
add that to our previous ray direction. Finally, we'll use the standard library's `accumulate` function to sum these
colors together, and then we'll divide the resulting color by the number of samples, effectively averaging our array.

```cpp
// ---✂--- Include the numeric header:

#include <numeric>

// ---✂--- Replace the current innermost loop contents in OnUserUpdate() with:

// Create an array of colors - we'll be sampling this pixel multiple
// times with varying offsets to create a multisample, and then
// rendering the average of these samples.
std::array<color3, SAMPLES> samples;

// For each sample...
for (auto i = 0; i < SAMPLES; i++) {
	// Create random offset within this pixel
	float offsetX = rand() / (float)RAND_MAX;
	float offsetY = rand() / (float)RAND_MAX;

	// Sample the color at that offset (converting screen coordinates to
	// scene coordinates).
	samples[i] = Sample(x - HALF_WIDTH + offsetX, y - HALF_HEIGHT + offsetY);
}

// Calculate the average color and draw it.
color3 color = std::accumulate(samples.begin(), samples.end(), color3()) / SAMPLES;
Draw(x, y, olc::PixelF(color.x, color.y, color.z));
```

<div class="note" markdown=1>
<div class="note-title">Note</div>

Running our project now will display a multisampled scene. **However**, note that since we calculate our sample
offsets randomly the edges of different features will flicker frame to frame as the average is recalculated.

</div>

To remedy this, we can increase the number of samples, though this increases the number of rays we need to calculate,
and so slows down our frame times. I've placed my constant defining the number of samples within the same preprocessor
if as my reflection count to help keep debug runs at an acceptable pace.

```diff
-constexpr int SAMPLES = 3;

 #ifdef DEBUG
 constexpr int BOUNCES = 2;
+constexpr int SAMPLES = 2;
 #else
 constexpr int BOUNCES = 5;
+constexpr int SAMPLES = 4;
 #endif
```

<div class="note" markdown=1>
<div class="note-title">Note</div>

Running our project now will display a properly multisampled scene. The multisampling will be more stable when running
in Release mode.
{% capture multisampling %}{{ "/images/2022-03-23-raytracing/14-Add multisampling.png" | absolute_url }}{% endcapture %}
{% capture multisampling_thumb %}{{ "/images/2022-03-23-raytracing/thumbs/th-14-Add multisampling.png" | absolute_url }}{% endcapture %}
{% include popimg.html src=multisampling thumb=multisampling_thumb alt="Multisampling" %}

<details>
<summary>Open Image Comparison</summary>
{% include imgcomp.html asrc=shadow_thumb aalt="" bsrc=multisampling_thumb balt="" width="471px" height="500px" %}
</details>

</div>

## All Done!

Check out the [GitHub repo][gh] to see the complete project.

[OLC]: https://community.onelonecoder.com/
[javid]: https://www.youtube.com/channel/UC-yuWVUplUJZvieEligKBkA
[gh]: https://github.com/Sidneys1/OlcPixelRayTracer
[olc-pge-header]: https://github.com/OneLoneCoder/olcPixelGameEngine/releases/latest/download/olcPixelGameEngine.h]]></content><author><name>Sidneys1</name></author><category term="programming" /><category term="programming" /><category term="raytracing" /><summary type="html"><![CDATA[Since I started programming I’ve had a dream in the back of my mind: raytracers are super cool, and I’d like to build one myself. But with that dream accompanied another thought: raytracers are nearly a pure expression of math, a discipline I am poorly qualified for. However this winter I discovered a new programming community, OneLoneCoder, and its leader, javidx9. Watching the videos produced by javidx9 inspired me to take a leap of faith in myself and start this raytracing project. The result has been amazing to see unfold as I developed first a working prototype in C#, then in C++, and finally as I produced what hopefully is an easy to follow “tutorial” style Git repository. So, lets dive in!]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://sidneys1.github.io/images/2022-03-23-raytracing/hero.png" /><media:content medium="image" url="https://sidneys1.github.io/images/2022-03-23-raytracing/hero.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>